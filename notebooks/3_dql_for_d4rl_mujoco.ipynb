{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tutorial 3. DQL (Diffusion Q-learning) for D4RL-MuJoCo**\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "The first two tutorials were pretty standard supervised learning tasks where we just provided the model with input and it started training. In this tutorial, we're going to tackle a more complex reinforcement learning task that involves alternating updates of a diffusion model and a Q function, which makes it tricky to handle using simple supervised learning methods. DQL is a simple and effective diffusion-based reinforcement learning actor-critic algorithm. Similar to TD3BC, its policy updates involve two types of loss: diffusion loss and Q maximizing loss, in which the former is just a behavior cloning loss:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal L_{\\text{policy}}(\\theta)=\\mathcal L_{\\text{diffusion}}(\\theta)-\\eta\\cdot Q_\\phi(\\bm s, \\pi_\\theta(\\bm a|\\bm s)). \\tag{1}\n",
    "\\end{equation}\n",
    "$$\n",
    "Its critic is a policy Q function that's trained using TD learning. \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal L_{\\text{critic}}(\\phi)=\\left(Q_\\phi(\\bm s, \\bm a)-(r + \\gamma\\cdot Q_{\\phi^-}(\\bm s', \\pi_\\theta(\\bm a'|\\bm s')))\\right)^2. \\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    " \n",
    "As a typical actor-critic algorithm, training of the diffusion model affects the Q function, and training of the Q function also impacts the diffusion model.\n",
    "\n",
    "## 2 Setting up the Environment and the Dataset\n",
    "\n",
    "Here we use `halfcheetah-medium-v2` in D4RL-MuJoCo as an example. D4RL-MuJoCo is a widely used offline RL benchmark, and `halfcheetah-medium-v2` requires to control a halfcheetah robot to move forward as fast as possible. CleanDiffuser has already provided a simple interface to load D4RL datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzb/miniforge3/envs/cleandiffuser/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "load datafile:   0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 21/21 [00:02<00:00,  8.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import d4rl\n",
    "import gym\n",
    "\n",
    "from cleandiffuser.dataset.d4rl_mujoco_dataset import D4RLMuJoCoTDDataset\n",
    "\n",
    "env = gym.make(\"halfcheetah-medium-v2\")\n",
    "dataset = D4RLMuJoCoTDDataset(d4rl.qlearning_dataset(env), normalize_reward=True)\n",
    "obs_dim, act_dim = dataset.obs_dim, dataset.act_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Building the Diffusion Model\n",
    "\n",
    "Diffusion model in DQL plays the role of a policy network. Just like in the tutorial 1, the diffusion model should be designed to generate actions conditioned on the input states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from cleandiffuser.diffusion import DiscreteDiffusionSDE\n",
    "from cleandiffuser.nn_diffusion import IDQLMlp\n",
    "from cleandiffuser.nn_condition import MLPCondition\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "nn_diffusion = IDQLMlp(x_dim=act_dim, emb_dim=64, timestep_emb_type=\"positional\")\n",
    "\n",
    "# The label dropout rate is set to 0.0 as we do not use Classifier-free Guidance.\n",
    "nn_condition = MLPCondition(in_dim=obs_dim, out_dim=64, hidden_dims=64, dropout=0.0)\n",
    "\n",
    "actor = DiscreteDiffusionSDE(\n",
    "    nn_diffusion,\n",
    "    nn_condition,\n",
    "    diffusion_steps=5,\n",
    "    x_max=torch.full((act_dim,), fill_value=1.0),\n",
    "    x_min=torch.full((act_dim,), fill_value=-1.0),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Q functions with LayerNorm and Mish activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class TwinQ(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.Q1 = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        self.Q2 = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def both(self, obs, act):\n",
    "        q1, q2 = self.Q1(torch.cat([obs, act], -1)), self.Q2(torch.cat([obs, act], -1))\n",
    "        return q1, q2\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        return torch.min(*self.both(obs, act))\n",
    "\n",
    "\n",
    "critic = TwinQ(obs_dim, act_dim, hidden_dim=256).to(device)\n",
    "critic_target = deepcopy(critic).requires_grad_(False).eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Training the Diffusion Model in Manual-update Style\n",
    "\n",
    "To implement complex model update steps, we can use `update_diffusion` for manual updates, allowing for more flexible update logic. This way, we don’t need to add a wrapper to the dataset to match the format required by PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000] {'bc_loss': 0.12377934498339892, 'policy_q_loss': -0.9846072461605072, 'critic_td_loss': 7.595048444822431, 'target_q': 18.960346080839635}\n",
      "[2000] {'bc_loss': 0.08238512057811022, 'policy_q_loss': -0.98832589417696, 'critic_td_loss': 8.06298545050621, 'target_q': 40.818730871200565}\n",
      "[3000] {'bc_loss': 0.06406415082886815, 'policy_q_loss': -0.9965834428668022, 'critic_td_loss': 3.769754119157791, 'target_q': 81.05738888549804}\n",
      "[4000] {'bc_loss': 0.060024288821965456, 'policy_q_loss': -0.9959487068653107, 'critic_td_loss': 2.715253163576126, 'target_q': 88.40225748443603}\n",
      "[5000] {'bc_loss': 0.057553786966949703, 'policy_q_loss': -0.9958168971538544, 'critic_td_loss': 2.2044705004692076, 'target_q': 89.54628697967529}\n",
      "[6000] {'bc_loss': 0.055614226009696725, 'policy_q_loss': -0.9962961921691894, 'critic_td_loss': 2.152037105202675, 'target_q': 90.65434854888916}\n",
      "[7000] {'bc_loss': 0.05411395792290569, 'policy_q_loss': -0.9968310383558273, 'critic_td_loss': 2.0946031621694563, 'target_q': 91.00877456665039}\n",
      "[8000] {'bc_loss': 0.053040192823857066, 'policy_q_loss': -0.9970581630468368, 'critic_td_loss': 2.089537423491478, 'target_q': 91.09306433868409}\n",
      "[9000] {'bc_loss': 0.05220864191278815, 'policy_q_loss': -0.9972892431020737, 'critic_td_loss': 1.9933094004392624, 'target_q': 90.94313095092774}\n",
      "[10000] {'bc_loss': 0.05156139689683914, 'policy_q_loss': -0.997428586423397, 'critic_td_loss': 1.9899939378499985, 'target_q': 90.83453469848632}\n",
      "[11000] {'bc_loss': 0.05088695199042559, 'policy_q_loss': -0.9975190041065216, 'critic_td_loss': 2.0077701959609984, 'target_q': 90.55402744293212}\n",
      "[12000] {'bc_loss': 0.0503754466958344, 'policy_q_loss': -0.9976012971401215, 'critic_td_loss': 2.0875644409656524, 'target_q': 90.60097731781006}\n",
      "[13000] {'bc_loss': 0.04988188029080629, 'policy_q_loss': -0.9975700889229775, 'critic_td_loss': 1.9234897087812424, 'target_q': 90.75039376831054}\n",
      "[14000] {'bc_loss': 0.04935646006464958, 'policy_q_loss': -0.9976271979212761, 'critic_td_loss': 1.9469688965082168, 'target_q': 90.5563193511963}\n",
      "[15000] {'bc_loss': 0.04912726059183478, 'policy_q_loss': -0.9975976002216339, 'critic_td_loss': 1.993091944217682, 'target_q': 90.75858070373535}\n",
      "[16000] {'bc_loss': 0.04883867771551013, 'policy_q_loss': -0.9976327270269394, 'critic_td_loss': 1.830213016152382, 'target_q': 90.58212157440185}\n",
      "[17000] {'bc_loss': 0.048479839108884336, 'policy_q_loss': -0.9977630069851875, 'critic_td_loss': 1.9759398952722549, 'target_q': 90.51173374176025}\n",
      "[18000] {'bc_loss': 0.04822055233642459, 'policy_q_loss': -0.9977211248278618, 'critic_td_loss': 1.9057118853330612, 'target_q': 90.3163706741333}\n",
      "[19000] {'bc_loss': 0.048006226498633627, 'policy_q_loss': -0.997807525575161, 'critic_td_loss': 1.9110087531805038, 'target_q': 90.89287877655029}\n",
      "[20000] {'bc_loss': 0.04772239149361849, 'policy_q_loss': -0.9978301274180412, 'critic_td_loss': 1.8956707329750062, 'target_q': 91.22640341186523}\n",
      "[21000] {'bc_loss': 0.04749045519903302, 'policy_q_loss': -0.9978733542561531, 'critic_td_loss': 1.884393355369568, 'target_q': 91.02763413238526}\n",
      "[22000] {'bc_loss': 0.0472826703414321, 'policy_q_loss': -0.9978316285014153, 'critic_td_loss': 1.8497930525541306, 'target_q': 90.94022792816162}\n",
      "[23000] {'bc_loss': 0.047075526464730504, 'policy_q_loss': -0.9978941125869751, 'critic_td_loss': 1.865306748151779, 'target_q': 91.38294580841064}\n",
      "[24000] {'bc_loss': 0.04690831158310175, 'policy_q_loss': -0.9978852670192718, 'critic_td_loss': 1.8794479244947433, 'target_q': 91.21482678985596}\n",
      "[25000] {'bc_loss': 0.046801236532628535, 'policy_q_loss': -0.9978882454037666, 'critic_td_loss': 1.9143556036949159, 'target_q': 91.74752275085449}\n",
      "[26000] {'bc_loss': 0.04660053424909711, 'policy_q_loss': -0.9978941105008126, 'critic_td_loss': 1.8549622435569764, 'target_q': 91.43288809204101}\n",
      "[27000] {'bc_loss': 0.046590707153081896, 'policy_q_loss': -0.9978550778627395, 'critic_td_loss': 1.9267259372472763, 'target_q': 91.59356067657471}\n",
      "[28000] {'bc_loss': 0.046255142502486704, 'policy_q_loss': -0.9979201868772507, 'critic_td_loss': 1.8861933938264848, 'target_q': 91.65143508148194}\n",
      "[29000] {'bc_loss': 0.046269795905798675, 'policy_q_loss': -0.9978989880681038, 'critic_td_loss': 1.8043409082889557, 'target_q': 91.89596761322022}\n",
      "[30000] {'bc_loss': 0.04616234628856182, 'policy_q_loss': -0.9978736659884453, 'critic_td_loss': 1.8401020979881286, 'target_q': 92.11229150390625}\n",
      "[31000] {'bc_loss': 0.04604028583317995, 'policy_q_loss': -0.99788740503788, 'critic_td_loss': 1.9087075167894363, 'target_q': 92.32553395080566}\n",
      "[32000] {'bc_loss': 0.04588197275623679, 'policy_q_loss': -0.9978796949386597, 'critic_td_loss': 1.828899073600769, 'target_q': 92.68799985504151}\n",
      "[33000] {'bc_loss': 0.04565194182097912, 'policy_q_loss': -0.9977437697649002, 'critic_td_loss': 1.8333785482645035, 'target_q': 92.33671680450439}\n",
      "[34000] {'bc_loss': 0.04568578660488129, 'policy_q_loss': -0.9978132528662682, 'critic_td_loss': 1.8168689839839935, 'target_q': 92.37212291717529}\n",
      "[35000] {'bc_loss': 0.04555473426729441, 'policy_q_loss': -0.9977719869613647, 'critic_td_loss': 1.8601192625761032, 'target_q': 92.33001644134521}\n",
      "[36000] {'bc_loss': 0.04549960194528103, 'policy_q_loss': -0.9978073639273644, 'critic_td_loss': 1.790262443780899, 'target_q': 92.70445469665528}\n",
      "[37000] {'bc_loss': 0.045304747976362705, 'policy_q_loss': -0.9977950496077538, 'critic_td_loss': 1.861409428715706, 'target_q': 92.95817888641358}\n",
      "[38000] {'bc_loss': 0.045244823902845385, 'policy_q_loss': -0.9977617614865303, 'critic_td_loss': 1.7984198620319367, 'target_q': 92.50922239685059}\n",
      "[39000] {'bc_loss': 0.045099192898720504, 'policy_q_loss': -0.9978001798391343, 'critic_td_loss': 1.8455589238405228, 'target_q': 92.89445722961426}\n",
      "[40000] {'bc_loss': 0.045159375436604025, 'policy_q_loss': -0.9977756344676018, 'critic_td_loss': 1.8493853187561036, 'target_q': 93.30978244781494}\n",
      "[41000] {'bc_loss': 0.04492853917554021, 'policy_q_loss': -0.9977436648011208, 'critic_td_loss': 1.7871884051561355, 'target_q': 92.95382106018066}\n",
      "[42000] {'bc_loss': 0.04497796566784382, 'policy_q_loss': -0.9977697737812996, 'critic_td_loss': 1.8260749295949936, 'target_q': 93.3710888519287}\n",
      "[43000] {'bc_loss': 0.04491821127384901, 'policy_q_loss': -0.9977583471536636, 'critic_td_loss': 1.7329832540750503, 'target_q': 93.14523943328858}\n"
     ]
    }
   ],
   "source": [
    "from cleandiffuser.utils import loop_dataloader, FreezeModules\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "save_path = \"../results/tutorial3_dql_for_d4rl_mujoco/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "batch_size = 2048\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True, drop_last=True\n",
    ")\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=3e-4)\n",
    "actor.configure_manual_optimizers()\n",
    "\n",
    "step = 0\n",
    "log = dict.fromkeys([\"bc_loss\", \"policy_q_loss\", \"critic_td_loss\", \"target_q\"], 0.0)\n",
    "prior = torch.zeros((batch_size, act_dim), device=device)\n",
    "\n",
    "for batch in loop_dataloader(dataloader):\n",
    "    obs, next_obs = batch[\"obs\"][\"state\"].to(device), batch[\"next_obs\"][\"state\"].to(device)\n",
    "    act = batch[\"act\"].to(device)\n",
    "    rew = batch[\"rew\"].to(device)\n",
    "    tml = batch[\"tml\"].to(device)\n",
    "\n",
    "    # --- Critic Update ---\n",
    "    actor.eval()\n",
    "    critic.train()\n",
    "\n",
    "    q1, q2 = critic.both(obs, act)\n",
    "\n",
    "    next_act, _ = actor.sample(\n",
    "        prior,\n",
    "        solver=\"ddpm\",\n",
    "        n_samples=batch_size,\n",
    "        sample_steps=5,\n",
    "        condition_cfg=next_obs,\n",
    "        w_cfg=1.0,\n",
    "        requires_grad=False,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target_q = torch.min(*critic_target.both(next_obs, next_act))\n",
    "\n",
    "    target_q = (rew + (1 - tml) * 0.99 * target_q).detach()\n",
    "\n",
    "    critic_td_loss = (q1 - target_q).pow(2).mean() + (q2 - target_q).pow(2).mean()\n",
    "\n",
    "    critic_optim.zero_grad()\n",
    "    critic_td_loss.backward()\n",
    "    critic_optim.step()\n",
    "\n",
    "    log[\"critic_td_loss\"] += critic_td_loss.item()\n",
    "    log[\"target_q\"] += target_q.mean().item()\n",
    "\n",
    "    # --- Actor Update ---\n",
    "    actor.train()\n",
    "    critic.eval()\n",
    "\n",
    "    bc_loss = actor.loss(act, obs)\n",
    "\n",
    "    new_act, _ = actor.sample(\n",
    "        prior,\n",
    "        solver=\"ddpm\",\n",
    "        n_samples=batch_size,\n",
    "        sample_steps=5,\n",
    "        condition_cfg=obs,\n",
    "        w_cfg=1.0,\n",
    "        use_ema=False,\n",
    "        requires_grad=True,\n",
    "    )\n",
    "\n",
    "    with FreezeModules([critic]):\n",
    "        q1_actor, q2_actor = critic.both(obs, new_act)\n",
    "\n",
    "    if np.random.uniform() > 0.5:\n",
    "        policy_q_loss = -q1_actor.mean() / q2_actor.abs().mean().detach()\n",
    "    else:\n",
    "        policy_q_loss = -q2_actor.mean() / q1_actor.abs().mean().detach()\n",
    "\n",
    "    # eta=1.0 for halfcheetah-medium-v2\n",
    "    actor_loss = bc_loss + policy_q_loss * 1.0\n",
    "\n",
    "    actor.manual_optimizers[\"diffusion\"].zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor.manual_optimizers[\"diffusion\"].step()\n",
    "\n",
    "    log[\"bc_loss\"] += bc_loss.item()\n",
    "    log[\"policy_q_loss\"] += policy_q_loss.item()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    # --- EMA Update ---\n",
    "    if step % 5 == 0:\n",
    "        if step >= 1000:\n",
    "            actor.ema_update()\n",
    "        for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
    "            target_param.data.copy_(0.995 * param.data + (1 - 0.995) * target_param.data)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        log = {k: v / 1000 for k, v in log.items()}\n",
    "        print(f\"[{step}] {log}\")\n",
    "        log = dict.fromkeys([\"bc_loss\", \"policy_q_loss\", \"critic_td_loss\", \"target_q\"], 0.0)\n",
    "\n",
    "    if step % 50_000 == 0:\n",
    "        actor.save(save_path + f\"actor_step={step}.ckpt\")\n",
    "        torch.save(critic.state_dict(), save_path + f\"critic_step={step}.ckpt\")\n",
    "        torch.save(critic_target.state_dict(), save_path + f\"critic_target_step={step}.ckpt\")\n",
    "\n",
    "    if step >= 200_000:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleandiffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
